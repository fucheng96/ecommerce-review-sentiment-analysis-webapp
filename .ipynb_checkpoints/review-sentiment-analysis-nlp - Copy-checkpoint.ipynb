{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6c047a",
   "metadata": {},
   "source": [
    "# Project Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6477c79",
   "metadata": {},
   "source": [
    "The main objective is to perform Sentiment Analysis on customer reviews on a Brazilian E-Commerce platform using Natural Language Processing ('NLP'), to determine if the reviews are positive or negative overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f082303",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94ff15e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\fucheng.liew\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fdad4f2c4aa8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# For visualization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# import seaborn as sns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 174\u001b[1;33m \u001b[0m_check_versions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m_check_versions\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;31m# Quickfix to ensure Microsoft Visual C++ redistributable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;31m# DLLs are loaded before importing kiwisolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mft2font\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     for modname, minver in [\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ft2font' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\fucheng.liew\\Anaconda3\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "# For visualization\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Import nltk libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(['punkt', 'wordnet', 'stopwords', 'averaged_perceptron_tagger'])\n",
    "\n",
    "# Import SQLite libraries\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import sklearn libraries\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch, torchvision\n",
    "print(torch.__version__) # Check PyTorch version\n",
    "\n",
    "# PyTorch to use CPU instead of GPU\n",
    "torch.cuda.is_available = lambda : False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb5d17d",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a1e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view all generated results\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "# Widen the Jupyter Notebook as much as possible\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8382816d",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ef9bf",
   "metadata": {},
   "source": [
    "***The Brazilian E-Commerce Public Dataset by [Olist](https://olist.com/pt-br/)***: https://www.kaggle.com/olistbr/brazilian-ecommerce<br>\n",
    "The dataset contains information of 100k orders from 2016 to 2018 such as order status, price, payment and freight performance to customer location, product attributes, etc.<br><br>\n",
    "The focal point here would be the **reviews written by the customers**.<br>\n",
    "Once the customer receives the product, or when the estimated delivery date is due (whether customer receives the product or not), the customer gets a satisfaction survey by email to describe the purchase experience and write down some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e4383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the directories\n",
    "cd = os.getcwd()\n",
    "data_dir = cd + '\\\\data'\n",
    "\n",
    "# Import data\n",
    "review_df = pd.read_csv(data_dir + '\\\\olist_order_reviews_dataset.csv')\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd601e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset info\n",
    "review_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0647d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View any missing values\n",
    "review_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8bb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other key features such as product category and location to discover more about the reviews\n",
    "order_df = pd.read_csv(data_dir + '\\\\olist_orders_dataset.csv') # Includes data on all the orders made\n",
    "order_item_df = pd.read_csv(data_dir + '\\\\olist_order_items_dataset.csv') # Includes data about the items purchased within each order\n",
    "prod_df = pd.read_csv(data_dir + '\\\\olist_products_dataset.csv') # Includes data about the products sold by Olist\n",
    "prod_trans_df = pd.read_csv(data_dir + '\\\\product_category_name_translation.csv') # Translates the productcategoryname to english.\n",
    "cust_df = pd.read_csv(data_dir + '\\\\olist_customers_dataset.csv') # Includes data about the customer and its location\n",
    "\n",
    "# Show samples, dataset info and missing values info\n",
    "df_list = [order_df, order_item_df, prod_df, prod_trans_df, cust_df]\n",
    "for df_ in df_list:\n",
    "    # Show samples\n",
    "    print(df_.head())\n",
    "    print('\\n')\n",
    "    \n",
    "    # Show dataset info\n",
    "    print(df_.info())\n",
    "    print('\\n')\n",
    "    \n",
    "    # Show missing values info\n",
    "    print(df_.isnull().sum())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6738c9",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62aeb06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep key columns\n",
    "df = review_df[['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message']]\n",
    "\n",
    "# Remove missing reviews from review dataset\n",
    "df = df[~df['review_comment_message'].isnull()]\n",
    "len(df) # Around 41k reviews remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ff6257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicates\n",
    "len(df[df.duplicated()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a20f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicates based on review_id\n",
    "len(df[df.duplicated(subset='review_id')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad8ccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View samples of duplicated review_id\n",
    "df[df.duplicated(subset='review_id')].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99bde88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View dataset of first 2 samples\n",
    "df[df['review_id'].isin(['3242cc306a9218d0377831e175d62fbf', '308316408775d1600dad81bd3184556d'])]\n",
    "\n",
    "# The review_id has duplicates due to the same review for multiple products ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9664f47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any duplicates based on review_id and review_comment_message\n",
    "len(df[df.duplicated(subset=['review_id', 'review_comment_message'])])\n",
    "\n",
    "# This duplicate count here is same as that based on review_id, which needs to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3728f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "df.drop_duplicates(subset=['review_id', 'review_comment_message'], inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c14bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter key columns\n",
    "order_df = order_df[['order_id', 'customer_id', 'order_delivered_customer_date', 'order_estimated_delivery_date']]\n",
    "order_item_df = order_item_df[['order_id', 'product_id']]\n",
    "prod_df = prod_df[['product_id', 'product_category_name']]\n",
    "cust_df = cust_df[['customer_id', 'customer_city', 'customer_state']]\n",
    "\n",
    "# List each dataset and its key ID for data merging\n",
    "df_list_updated = [order_df, order_item_df, prod_df, prod_trans_df, cust_df]\n",
    "df_key_id_list = ['order_id', 'order_id', 'product_id', 'product_category_name', 'customer_id']\n",
    "\n",
    "# Remove duplicates and merge with other datasets to extract key features\n",
    "print(len(df)) # Get dataset row count before merging to check for any duplicates\n",
    "\n",
    "i = 0 # Set counter to loop through the list\n",
    "for df_ in df_list_updated:\n",
    "    \n",
    "    # Get respective key id from df_key_id_list\n",
    "    id_ = df_key_id_list[i]\n",
    "    print(id_)\n",
    "    \n",
    "    # Remove duplicates before merging\n",
    "    print('Merge with df using ' + id_ + ':')\n",
    "    print('\\n')\n",
    "    df_.drop_duplicates(subset=[id_], inplace=True)\n",
    "    df = df.merge(df_, on=id_, how='left')\n",
    "    \n",
    "    i += 1 # Update counter\n",
    "    \n",
    "print(len(df)) # The dataset row count remain unchanged after merging, dataset shape remained intact\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View any missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e464d4e",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40aed7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine if the delivery was late or never arrived (missing order_delivered_customer_date is assumed to never arrive)\n",
    "df[['order_estimated_delivery_date', 'order_delivered_customer_date']] = df[['order_estimated_delivery_date', 'order_delivered_customer_date']].astype(np.datetime64)\n",
    "df['delivery_late_ind'] = 0\n",
    "df.loc[(df['order_delivered_customer_date'].isnull()) | \n",
    "       (df['order_delivered_customer_date'] > df['order_estimated_delivery_date'] + pd.Timedelta(days=1)), 'delivery_late_ind'] = 1\n",
    "\n",
    "# View distribution\n",
    "print(df['delivery_late_ind'].value_counts().sort_index())\n",
    "\n",
    "# View data samples\n",
    "df[df['delivery_late_ind'] > 0].head() \n",
    "\n",
    "# First review message is all capitalized, expressing dissatisfaction\n",
    "# However, only a small % of reviews are due to late deliveries, may not be a good feature in distinguishing positive / negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77209185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine if entire review_comment_message is capitalized\n",
    "df['capitalize_ind'] = 0\n",
    "df.loc[df['review_comment_message'] == df['review_comment_message'].str.upper(), 'capitalize_ind'] = 1\n",
    "\n",
    "# View distribution\n",
    "print(df['capitalize_ind'].value_counts().sort_index())\n",
    "\n",
    "# View data samples\n",
    "df[df['capitalize_ind'] > 0].head()\n",
    "\n",
    "# Only a small % of reviews are fully capitalized, may not be a good feature in distinguishing positive / negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a7a935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine length of review_comment_message\n",
    "df['review_message_length'] = df['review_comment_message'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# View distribution\n",
    "print(df['review_message_length'].value_counts().sort_index())\n",
    "\n",
    "# View data samples\n",
    "df[df['review_message_length'] <= 3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1097f8",
   "metadata": {},
   "source": [
    "View breakdown of attributes and the review score given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2039d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attribute_list = ['product_category_name_english', 'customer_city', 'customer_state', \n",
    "                  'delivery_late_ind', 'review_message_length', 'capitalize_ind']\n",
    "\n",
    "for att in attribute_list:\n",
    "    # Generate the average score\n",
    "    avg_score_table = pd.pivot_table(df, index=att, values='review_score', aggfunc=np.mean, fill_value=0).reset_index()\n",
    "    \n",
    "    # Get the distribution of scores,\n",
    "    score_dist_table = pd.pivot_table(df, index=att, columns='review_score', values='review_id', aggfunc='count', fill_value=0).reset_index()\n",
    "    score_count_total = pd.pivot_table(df, index=att, values='review_id', aggfunc='count', fill_value=0).reset_index()\n",
    "    score_dist_table = score_dist_table.merge( score_count_total, on=att, how='left')\n",
    "    score_dist_table = score_dist_table.set_index(att)\n",
    "    score_dist_perc_table = score_dist_table.div(score_dist_table.iloc[:,-1], axis=0)\n",
    "    \n",
    "    # Merge the data together\n",
    "    combined_table = score_dist_perc_table.merge(avg_score_table, on=att, how='left')\n",
    "    \n",
    "    # Show table\n",
    "    print(att)\n",
    "    print(combined_table)\n",
    "    \n",
    "# From a quick glance, the review score tends to be higher if:\n",
    "# a. Review message is shorter\n",
    "# b. Delivery is on time or earlier\n",
    "\n",
    "# Given that the focus is on the web app, only input would be the review text message itself\n",
    "# Thus, the city and product category will not be focused\n",
    "# However, these features will help enhance our understanding of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a96d72f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3c722",
   "metadata": {},
   "source": [
    "This section is focused on preparing the dataset based on the content of the comments itself, along with other relevant attributes disclosed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tokenizer function to extract tokens for Tfidf transformer after this\n",
    "def tokenizer(text):\n",
    "    # Remove punctuations \n",
    "    detected_punctuations = re.findall('[^a-zA-Z0-9]', text)\n",
    "    for punctuation in detected_punctuations:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "        \n",
    "    # Remove words with single letters\n",
    "    text = ' '.join([w for w in text.split() if len(w) > 1])\n",
    "        \n",
    "    # Tokenize the words\n",
    "    tokens = word_tokenize(text)    \n",
    "    \n",
    "    # Lemmanitizer to reduce words to its stems\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # List of clean tokens\n",
    "    clean_tokens = [lemmatizer.lemmatize(w).lower().strip() for w in tokens]\n",
    "    \n",
    "    # Remove stopwords in Portugese\n",
    "    por_stopwords = stopwords.words('portuguese')\n",
    "    for st in por_stopwords:\n",
    "        if st in clean_tokens:\n",
    "            clean_tokens.remove(st)\n",
    "            \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20746898",
   "metadata": {},
   "source": [
    "Creating the response variable of whether the review is positive or negative based on review score with:<br>\n",
    "positive_review_ind = 1 if score is 4 or 5; else = 0<br><br>\n",
    "This assumption is necessary for supervised learning to enable the model to learn if it's a positive review or not.<br>\n",
    "Due to time constraint, each review_comment_message is not manually assessed to label it as positive or not.<br>\n",
    "However, this assumption is reasonable as customers would tend to write positive review given a high score (4 or 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e149a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the response variable\n",
    "df['positive_review_ind'] = 0\n",
    "df.loc[df['review_score'] >= 4, 'positive_review_ind'] = 1\n",
    "\n",
    "# View distribution\n",
    "print(df['review_score'].value_counts().sort_index())\n",
    "df['positive_review_ind'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f697d9f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626d63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the predictor and response datasets\n",
    "X = df['review_comment_message']#, 'product_category_name_english', 'customer_city', 'delivery_late_ind', 'review_message_length', 'capitalize_ind']]\n",
    "y = df['positive_review_ind']\n",
    "\n",
    "# Separate into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6521225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Based on attributes observed earlier, longer comments tend to associate with negative sentiments (lower review score)\n",
    "class GetReviewLength(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_len = pd.Series(X).apply(lambda x: len(x.split()))\n",
    "        return pd.DataFrame(X_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9754392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create ML pipeline - Random Forest\n",
    "pipeline = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenizer)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('review_length', GetReviewLength())\n",
    "        ])),\n",
    "\n",
    "        ('clf', RandomForestClassifier(n_jobs=-1, verbose=2)) # Use all processors\n",
    "    ])\n",
    "\n",
    "# Train the model using grid search\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba5ebebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(tokenizer=<function tokenizer at 0x0000021351B8D8B0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('review_length',\n",
       "                                                 GetReviewLength())])),\n",
       "                ('clf', AdaBoostClassifier())])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ML pipeline - AdaBoost\n",
    "pipeline2 = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenizer)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('review_length', GetReviewLength())\n",
    "        ])),\n",
    "\n",
    "        ('clf', AdaBoostClassifier())\n",
    "    ])\n",
    "\n",
    "# Train the model using grid search\n",
    "pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4cc1d224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2512           52.10s\n",
      "         2           1.2102           51.36s\n",
      "         3           1.1782           46.57s\n",
      "         4           1.1496           44.63s\n",
      "         5           1.1250           42.66s\n",
      "         6           1.1051           41.51s\n",
      "         7           1.0866           40.15s\n",
      "         8           1.0714           39.10s\n",
      "         9           1.0554           38.32s\n",
      "        10           1.0410           38.05s\n",
      "        11           1.0289           37.21s\n",
      "        12           1.0157           36.56s\n",
      "        13           1.0062           35.83s\n",
      "        14           0.9963           35.24s\n",
      "        15           0.9851           34.73s\n",
      "        16           0.9753           34.00s\n",
      "        17           0.9670           33.50s\n",
      "        18           0.9598           33.03s\n",
      "        19           0.9521           32.53s\n",
      "        20           0.9459           32.01s\n",
      "        21           0.9395           31.45s\n",
      "        22           0.9324           31.04s\n",
      "        23           0.9275           30.92s\n",
      "        24           0.9213           30.78s\n",
      "        25           0.9165           30.68s\n",
      "        26           0.9114           30.40s\n",
      "        27           0.9065           29.97s\n",
      "        28           0.9025           29.55s\n",
      "        29           0.8973           29.27s\n",
      "        30           0.8934           29.02s\n",
      "        31           0.8891           28.72s\n",
      "        32           0.8838           28.48s\n",
      "        33           0.8796           28.17s\n",
      "        34           0.8757           27.85s\n",
      "        35           0.8718           27.52s\n",
      "        36           0.8689           27.19s\n",
      "        37           0.8657           26.87s\n",
      "        38           0.8621           26.51s\n",
      "        39           0.8596           26.16s\n",
      "        40           0.8563           25.79s\n",
      "        41           0.8527           25.32s\n",
      "        42           0.8491           24.80s\n",
      "        43           0.8462           24.31s\n",
      "        44           0.8437           23.81s\n",
      "        45           0.8410           23.35s\n",
      "        46           0.8384           22.86s\n",
      "        47           0.8356           22.38s\n",
      "        48           0.8330           21.91s\n",
      "        49           0.8305           21.40s\n",
      "        50           0.8278           20.76s\n",
      "        51           0.8260           20.19s\n",
      "        52           0.8231           19.73s\n",
      "        53           0.8212           19.29s\n",
      "        54           0.8186           18.85s\n",
      "        55           0.8162           18.41s\n",
      "        56           0.8144           17.97s\n",
      "        57           0.8125           17.53s\n",
      "        58           0.8097           17.10s\n",
      "        59           0.8071           16.57s\n",
      "        60           0.8050           16.06s\n",
      "        61           0.8031           15.64s\n",
      "        62           0.8014           15.25s\n",
      "        63           0.7997           14.85s\n",
      "        64           0.7972           14.44s\n",
      "        65           0.7953           14.03s\n",
      "        66           0.7938           13.63s\n",
      "        67           0.7918           13.22s\n",
      "        68           0.7897           12.82s\n",
      "        69           0.7881           12.41s\n",
      "        70           0.7867           12.04s\n",
      "        71           0.7852           11.67s\n",
      "        72           0.7840           11.29s\n",
      "        73           0.7821           10.92s\n",
      "        74           0.7807           10.53s\n",
      "        75           0.7789           10.16s\n",
      "        76           0.7777            9.78s\n",
      "        77           0.7759            9.40s\n",
      "        78           0.7736            9.02s\n",
      "        79           0.7719            8.63s\n",
      "        80           0.7699            8.24s\n",
      "        81           0.7688            7.84s\n",
      "        82           0.7670            7.42s\n",
      "        83           0.7659            6.96s\n",
      "        84           0.7647            6.53s\n",
      "        85           0.7635            6.12s\n",
      "        86           0.7620            5.71s\n",
      "        87           0.7604            5.29s\n",
      "        88           0.7589            4.88s\n",
      "        89           0.7576            4.47s\n",
      "        90           0.7564            4.06s\n",
      "        91           0.7553            3.65s\n",
      "        92           0.7542            3.23s\n",
      "        93           0.7531            2.83s\n",
      "        94           0.7519            2.42s\n",
      "        95           0.7500            2.02s\n",
      "        96           0.7484            1.61s\n",
      "        97           0.7474            1.21s\n",
      "        98           0.7459            0.80s\n",
      "        99           0.7444            0.40s\n",
      "       100           0.7433            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(tokenizer=<function tokenizer at 0x0000021351B8D8B0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('review_length',\n",
       "                                                 GetReviewLength())])),\n",
       "                ('clf', GradientBoostingClassifier(verbose=2))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ML pipeline - Gradient Boosting\n",
    "pipeline3 = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenizer)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('review_length', GetReviewLength())\n",
    "        ])),\n",
    "\n",
    "        ('clf', GradientBoostingClassifier(verbose=2))\n",
    "    ])\n",
    "\n",
    "# Train the model using grid search\n",
    "pipeline3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b26901df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.46827144\n",
      "Iteration 2, loss = 0.29535110\n",
      "Iteration 3, loss = 0.25586749\n",
      "Iteration 4, loss = 0.23336080\n",
      "Iteration 5, loss = 0.21872221\n",
      "Iteration 6, loss = 0.20711621\n",
      "Iteration 7, loss = 0.19723907\n",
      "Iteration 8, loss = 0.18996318\n",
      "Iteration 9, loss = 0.18353546\n",
      "Iteration 10, loss = 0.17786924\n",
      "Iteration 11, loss = 0.17286173\n",
      "Iteration 12, loss = 0.17016895\n",
      "Iteration 13, loss = 0.16646619\n",
      "Iteration 14, loss = 0.16284918\n",
      "Iteration 15, loss = 0.15871599\n",
      "Iteration 16, loss = 0.15720155\n",
      "Iteration 17, loss = 0.15458850\n",
      "Iteration 18, loss = 0.15201395\n",
      "Iteration 19, loss = 0.14917835\n",
      "Iteration 20, loss = 0.14775034\n",
      "Iteration 21, loss = 0.14512442\n",
      "Iteration 22, loss = 0.14341034\n",
      "Iteration 23, loss = 0.14214968\n",
      "Iteration 24, loss = 0.13979895\n",
      "Iteration 25, loss = 0.13792400\n",
      "Iteration 26, loss = 0.13625243\n",
      "Iteration 27, loss = 0.13441570\n",
      "Iteration 28, loss = 0.13277934\n",
      "Iteration 29, loss = 0.12986178\n",
      "Iteration 30, loss = 0.12823776\n",
      "Iteration 31, loss = 0.12804123\n",
      "Iteration 32, loss = 0.12492805\n",
      "Iteration 33, loss = 0.12424779\n",
      "Iteration 34, loss = 0.12169173\n",
      "Iteration 35, loss = 0.12041274\n",
      "Iteration 36, loss = 0.11882676\n",
      "Iteration 37, loss = 0.11752529\n",
      "Iteration 38, loss = 0.11569940\n",
      "Iteration 39, loss = 0.11409542\n",
      "Iteration 40, loss = 0.11299994\n",
      "Iteration 41, loss = 0.11116842\n",
      "Iteration 42, loss = 0.10951875\n",
      "Iteration 43, loss = 0.10852156\n",
      "Iteration 44, loss = 0.10768423\n",
      "Iteration 45, loss = 0.10642318\n",
      "Iteration 46, loss = 0.10466856\n",
      "Iteration 47, loss = 0.10402096\n",
      "Iteration 48, loss = 0.10314599\n",
      "Iteration 49, loss = 0.10108161\n",
      "Iteration 50, loss = 0.10074036\n",
      "Iteration 51, loss = 0.09968344\n",
      "Iteration 52, loss = 0.09787215\n",
      "Iteration 53, loss = 0.09702996\n",
      "Iteration 54, loss = 0.09567301\n",
      "Iteration 55, loss = 0.09489855\n",
      "Iteration 56, loss = 0.09343797\n",
      "Iteration 57, loss = 0.09259652\n",
      "Iteration 58, loss = 0.09208898\n",
      "Iteration 59, loss = 0.09078065\n",
      "Iteration 60, loss = 0.09031190\n",
      "Iteration 61, loss = 0.08965467\n",
      "Iteration 62, loss = 0.08883232\n",
      "Iteration 63, loss = 0.08910690\n",
      "Iteration 64, loss = 0.08659260\n",
      "Iteration 65, loss = 0.08561452\n",
      "Iteration 66, loss = 0.08461943\n",
      "Iteration 67, loss = 0.08407253\n",
      "Iteration 68, loss = 0.08298049\n",
      "Iteration 69, loss = 0.08306004\n",
      "Iteration 70, loss = 0.08185614\n",
      "Iteration 71, loss = 0.08138212\n",
      "Iteration 72, loss = 0.08061875\n",
      "Iteration 73, loss = 0.07977221\n",
      "Iteration 74, loss = 0.07962560\n",
      "Iteration 75, loss = 0.07877310\n",
      "Iteration 76, loss = 0.07781672\n",
      "Iteration 77, loss = 0.07742379\n",
      "Iteration 78, loss = 0.07647546\n",
      "Iteration 79, loss = 0.07597505\n",
      "Iteration 80, loss = 0.07488638\n",
      "Iteration 81, loss = 0.07481628\n",
      "Iteration 82, loss = 0.07410363\n",
      "Iteration 83, loss = 0.07426693\n",
      "Iteration 84, loss = 0.07328291\n",
      "Iteration 85, loss = 0.07228176\n",
      "Iteration 86, loss = 0.07181716\n",
      "Iteration 87, loss = 0.07158646\n",
      "Iteration 88, loss = 0.07078582\n",
      "Iteration 89, loss = 0.07027773\n",
      "Iteration 90, loss = 0.06998535\n",
      "Iteration 91, loss = 0.06994735\n",
      "Iteration 92, loss = 0.06982855\n",
      "Iteration 93, loss = 0.06934011\n",
      "Iteration 94, loss = 0.06853811\n",
      "Iteration 95, loss = 0.06726105\n",
      "Iteration 96, loss = 0.06671401\n",
      "Iteration 97, loss = 0.06760389\n",
      "Iteration 98, loss = 0.06664279\n",
      "Iteration 99, loss = 0.06584212\n",
      "Iteration 100, loss = 0.06545961\n",
      "Iteration 101, loss = 0.06512062\n",
      "Iteration 102, loss = 0.06519440\n",
      "Iteration 103, loss = 0.06454899\n",
      "Iteration 104, loss = 0.06416979\n",
      "Iteration 105, loss = 0.06426986\n",
      "Iteration 106, loss = 0.06355096\n",
      "Iteration 107, loss = 0.06316364\n",
      "Iteration 108, loss = 0.06396904\n",
      "Iteration 109, loss = 0.06270845\n",
      "Iteration 110, loss = 0.06231701\n",
      "Iteration 111, loss = 0.06168814\n",
      "Iteration 112, loss = 0.06136650\n",
      "Iteration 113, loss = 0.06134871\n",
      "Iteration 114, loss = 0.06084552\n",
      "Iteration 115, loss = 0.06208000\n",
      "Iteration 116, loss = 0.06074154\n",
      "Iteration 117, loss = 0.06061298\n",
      "Iteration 118, loss = 0.06013452\n",
      "Iteration 119, loss = 0.05972919\n",
      "Iteration 120, loss = 0.05983434\n",
      "Iteration 121, loss = 0.05917655\n",
      "Iteration 122, loss = 0.05897304\n",
      "Iteration 123, loss = 0.05882443\n",
      "Iteration 124, loss = 0.05868728\n",
      "Iteration 125, loss = 0.05856401\n",
      "Iteration 126, loss = 0.05802616\n",
      "Iteration 127, loss = 0.05835609\n",
      "Iteration 128, loss = 0.05809483\n",
      "Iteration 129, loss = 0.05786061\n",
      "Iteration 130, loss = 0.05740926\n",
      "Iteration 131, loss = 0.05728299\n",
      "Iteration 132, loss = 0.05749451\n",
      "Iteration 133, loss = 0.05758850\n",
      "Iteration 134, loss = 0.05698046\n",
      "Iteration 135, loss = 0.05684253\n",
      "Iteration 136, loss = 0.05647397\n",
      "Iteration 137, loss = 0.05588022\n",
      "Iteration 138, loss = 0.05610421\n",
      "Iteration 139, loss = 0.05563134\n",
      "Iteration 140, loss = 0.05533412\n",
      "Iteration 141, loss = 0.05517324\n",
      "Iteration 142, loss = 0.05503744\n",
      "Iteration 143, loss = 0.05565345\n",
      "Iteration 144, loss = 0.05541676\n",
      "Iteration 145, loss = 0.05500643\n",
      "Iteration 146, loss = 0.05474452\n",
      "Iteration 147, loss = 0.05496934\n",
      "Iteration 148, loss = 0.05478928\n",
      "Iteration 149, loss = 0.05449057\n",
      "Iteration 150, loss = 0.05416974\n",
      "Iteration 151, loss = 0.05405006\n",
      "Iteration 152, loss = 0.05389883\n",
      "Iteration 153, loss = 0.05367236\n",
      "Iteration 154, loss = 0.05360918\n",
      "Iteration 155, loss = 0.05363199\n",
      "Iteration 156, loss = 0.05352560\n",
      "Iteration 157, loss = 0.05372014\n",
      "Iteration 158, loss = 0.05337068\n",
      "Iteration 159, loss = 0.05286485\n",
      "Iteration 160, loss = 0.05284743\n",
      "Iteration 161, loss = 0.05279672\n",
      "Iteration 162, loss = 0.05301106\n",
      "Iteration 163, loss = 0.05312820\n",
      "Iteration 164, loss = 0.05232745\n",
      "Iteration 165, loss = 0.05254550\n",
      "Iteration 166, loss = 0.05237518\n",
      "Iteration 167, loss = 0.05231987\n",
      "Iteration 168, loss = 0.05268550\n",
      "Iteration 169, loss = 0.05233738\n",
      "Iteration 170, loss = 0.05229862\n",
      "Iteration 171, loss = 0.05185269\n",
      "Iteration 172, loss = 0.05173537\n",
      "Iteration 173, loss = 0.05184045\n",
      "Iteration 174, loss = 0.05289911\n",
      "Iteration 175, loss = 0.05138511\n",
      "Iteration 176, loss = 0.05126240\n",
      "Iteration 177, loss = 0.05162999\n",
      "Iteration 178, loss = 0.05121896\n",
      "Iteration 179, loss = 0.05091257\n",
      "Iteration 180, loss = 0.05071793\n",
      "Iteration 181, loss = 0.05186055\n",
      "Iteration 182, loss = 0.05110335\n",
      "Iteration 183, loss = 0.05100329\n",
      "Iteration 184, loss = 0.05109579\n",
      "Iteration 185, loss = 0.05076864\n",
      "Iteration 186, loss = 0.05137030\n",
      "Iteration 187, loss = 0.05103449\n",
      "Iteration 188, loss = 0.05080086\n",
      "Iteration 189, loss = 0.05088044\n",
      "Iteration 190, loss = 0.05042533\n",
      "Iteration 191, loss = 0.05023986\n",
      "Iteration 192, loss = 0.05020847\n",
      "Iteration 193, loss = 0.05028102\n",
      "Iteration 194, loss = 0.05011041\n",
      "Iteration 195, loss = 0.05015023\n",
      "Iteration 196, loss = 0.05008204\n",
      "Iteration 197, loss = 0.05077874\n",
      "Iteration 198, loss = 0.05009364\n",
      "Iteration 199, loss = 0.04988048\n",
      "Iteration 200, loss = 0.04989550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fucheng.liew\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('text_pipeline',\n",
       "                                                 Pipeline(steps=[('vect',\n",
       "                                                                  CountVectorizer(tokenizer=<function tokenizer at 0x0000021351B8D8B0>)),\n",
       "                                                                 ('tfidf',\n",
       "                                                                  TfidfTransformer())])),\n",
       "                                                ('review_length',\n",
       "                                                 GetReviewLength())])),\n",
       "                ('clf', MLPClassifier(verbose=True))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ML pipeline - Neural Network\n",
    "pipeline4 = Pipeline([\n",
    "        ('features', FeatureUnion([\n",
    "\n",
    "            ('text_pipeline', Pipeline([\n",
    "                ('vect', CountVectorizer(tokenizer=tokenizer)),\n",
    "                ('tfidf', TfidfTransformer())\n",
    "            ])),\n",
    "\n",
    "            ('review_length', GetReviewLength())\n",
    "        ])),\n",
    "\n",
    "        ('clf', MLPClassifier(verbose=True))\n",
    "    ])\n",
    "\n",
    "# Train the model using grid search\n",
    "pipeline4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4f8ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   56.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  4.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect',\n",
       "                 CountVectorizer(tokenizer=<function tokenizer at 0x0000021351B8D8B0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('clf',\n",
       "                 StackingClassifier(estimators=[('rf',\n",
       "                                                 RandomForestClassifier()),\n",
       "                                                ('ab', AdaBoostClassifier())],\n",
       "                                    final_estimator=LogisticRegression(),\n",
       "                                    verbose=2))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For stacking ensemble classifier\n",
    "estimators = [\n",
    "    ('rf', RandomForestClassifier()), # Using the bagging technique\n",
    "    ('ab', AdaBoostClassifier()) # Using the boosting technique\n",
    "]\n",
    "\n",
    "# Create ML pipeline\n",
    "pipeline5 = Pipeline([\n",
    "        ('vect', CountVectorizer(tokenizer=tokenizer)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', StackingClassifier(estimators=estimators, \n",
    "                                   final_estimator=LogisticRegression(),\n",
    "                                   verbose=2))\n",
    "    ])\n",
    "\n",
    "# Train pipeline\n",
    "pipeline5.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74c7d279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   22.6s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    8.5s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   10.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   20.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    9.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   13.7s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   23.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    7.8s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    5.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:   10.0s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   14.1s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   10.3s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   12.9s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   17.9s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   12.4s finished\n",
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 1 of 200building tree 2 of 200\n",
      "building tree 3 of 200\n",
      "building tree 4 of 200\n",
      "\n",
      "building tree 5 of 200\n",
      "building tree 6 of 200building tree 7 of 200\n",
      "\n",
      "building tree 8 of 200\n",
      "building tree 9 of 200\n",
      "building tree 10 of 200\n",
      "building tree 11 of 200\n",
      "building tree 12 of 200\n",
      "building tree 13 of 200\n",
      "building tree 14 of 200\n",
      "building tree 15 of 200\n",
      "building tree 16 of 200\n",
      "building tree 17 of 200\n",
      "building tree 18 of 200\n",
      "building tree 19 of 200\n",
      "building tree 20 of 200\n",
      "building tree 21 of 200\n",
      "building tree 22 of 200\n",
      "building tree 23 of 200\n",
      "building tree 24 of 200\n",
      "building tree 25 of 200\n",
      "building tree 26 of 200\n",
      "building tree 27 of 200\n",
      "building tree 28 of 200\n",
      "building tree 29 of 200\n",
      "building tree 30 of 200\n",
      "building tree 31 of 200\n",
      "building tree 32 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    2.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 33 of 200building tree 34 of 200\n",
      "\n",
      "building tree 35 of 200\n",
      "building tree 36 of 200\n",
      "building tree 37 of 200\n",
      "building tree 38 of 200\n",
      "building tree 39 of 200\n",
      "building tree 40 of 200\n",
      "building tree 41 of 200\n",
      "building tree 42 of 200\n",
      "building tree 43 of 200\n",
      "building tree 44 of 200\n",
      "building tree 45 of 200\n",
      "building tree 46 of 200\n",
      "building tree 47 of 200\n",
      "building tree 48 of 200\n",
      "building tree 49 of 200\n",
      "building tree 50 of 200\n",
      "building tree 51 of 200\n",
      "building tree 52 of 200\n",
      "building tree 53 of 200\n",
      "building tree 54 of 200\n",
      "building tree 55 of 200\n",
      "building tree 56 of 200\n",
      "building tree 57 of 200\n",
      "building tree 58 of 200\n",
      "building tree 59 of 200\n",
      "building tree 60 of 200\n",
      "building tree 61 of 200\n",
      "building tree 62 of 200\n",
      "building tree 63 of 200\n",
      "building tree 64 of 200\n",
      "building tree 65 of 200\n",
      "building tree 66 of 200\n",
      "building tree 67 of 200\n",
      "building tree 68 of 200\n",
      "building tree 69 of 200\n",
      "building tree 70 of 200\n",
      "building tree 71 of 200\n",
      "building tree 72 of 200\n",
      "building tree 73 of 200\n",
      "building tree 74 of 200\n",
      "building tree 75 of 200\n",
      "building tree 76 of 200\n",
      "building tree 77 of 200\n",
      "building tree 78 of 200\n",
      "building tree 79 of 200\n",
      "building tree 80 of 200\n",
      "building tree 81 of 200\n",
      "building tree 82 of 200\n",
      "building tree 83 of 200\n",
      "building tree 84 of 200\n",
      "building tree 85 of 200\n",
      "building tree 86 of 200\n",
      "building tree 87 of 200\n",
      "building tree 88 of 200\n",
      "building tree 89 of 200\n",
      "building tree 90 of 200\n",
      "building tree 91 of 200\n",
      "building tree 92 of 200\n",
      "building tree 93 of 200\n",
      "building tree 94 of 200\n",
      "building tree 95 of 200\n",
      "building tree 96 of 200\n",
      "building tree 97 of 200\n",
      "building tree 98 of 200\n",
      "building tree 99 of 200\n",
      "building tree 100 of 200\n",
      "building tree 101 of 200\n",
      "building tree 102 of 200\n",
      "building tree 103 of 200\n",
      "building tree 104 of 200\n",
      "building tree 105 of 200\n",
      "building tree 106 of 200\n",
      "building tree 107 of 200\n",
      "building tree 108 of 200\n",
      "building tree 109 of 200\n",
      "building tree 110 of 200\n",
      "building tree 111 of 200\n",
      "building tree 112 of 200\n",
      "building tree 113 of 200\n",
      "building tree 114 of 200\n",
      "building tree 115 of 200\n",
      "building tree 116 of 200\n",
      "building tree 117 of 200\n",
      "building tree 118 of 200\n",
      "building tree 119 of 200\n",
      "building tree 120 of 200\n",
      "building tree 121 of 200\n",
      "building tree 122 of 200\n",
      "building tree 123 of 200\n",
      "building tree 124 of 200\n",
      "building tree 125 of 200\n",
      "building tree 126 of 200\n",
      "building tree 127 of 200\n",
      "building tree 128 of 200\n",
      "building tree 129 of 200\n",
      "building tree 130 of 200\n",
      "building tree 131 of 200\n",
      "building tree 132 of 200\n",
      "building tree 133 of 200\n",
      "building tree 134 of 200\n",
      "building tree 135 of 200\n",
      "building tree 136 of 200\n",
      "building tree 137 of 200\n",
      "building tree 138 of 200\n",
      "building tree 139 of 200\n",
      "building tree 140 of 200\n",
      "building tree 141 of 200\n",
      "building tree 142 of 200\n",
      "building tree 143 of 200\n",
      "building tree 144 of 200\n",
      "building tree 145 of 200\n",
      "building tree 146 of 200\n",
      "building tree 147 of 200\n",
      "building tree 148 of 200\n",
      "building tree 149 of 200\n",
      "building tree 150 of 200\n",
      "building tree 151 of 200\n",
      "building tree 152 of 200\n",
      "building tree 153 of 200\n",
      "building tree 154 of 200\n",
      "building tree 155 of 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   13.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 156 of 200\n",
      "building tree 157 of 200\n",
      "building tree 158 of 200\n",
      "building tree 159 of 200\n",
      "building tree 160 of 200\n",
      "building tree 161 of 200\n",
      "building tree 162 of 200\n",
      "building tree 163 of 200\n",
      "building tree 164 of 200\n",
      "building tree 165 of 200\n",
      "building tree 166 of 200\n",
      "building tree 167 of 200\n",
      "building tree 168 of 200\n",
      "building tree 169 of 200\n",
      "building tree 170 of 200\n",
      "building tree 171 of 200\n",
      "building tree 172 of 200\n",
      "building tree 173 of 200\n",
      "building tree 174 of 200\n",
      "building tree 175 of 200\n",
      "building tree 176 of 200\n",
      "building tree 177 of 200\n",
      "building tree 178 of 200\n",
      "building tree 179 of 200\n",
      "building tree 180 of 200\n",
      "building tree 181 of 200\n",
      "building tree 182 of 200\n",
      "building tree 183 of 200\n",
      "building tree 184 of 200\n",
      "building tree 185 of 200\n",
      "building tree 186 of 200\n",
      "building tree 187 of 200\n",
      "building tree 188 of 200\n",
      "building tree 189 of 200\n",
      "building tree 190 of 200\n",
      "building tree 191 of 200\n",
      "building tree 192 of 200\n",
      "building tree 193 of 200\n",
      "building tree 194 of 200\n",
      "building tree 195 of 200\n",
      "building tree 196 of 200\n",
      "building tree 197 of 200\n",
      "building tree 198 of 200\n",
      "building tree 199 of 200\n",
      "building tree 200 of 200\n",
      "{'clf__min_samples_split': 2, 'clf__n_estimators': 200}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:   19.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Create ML pipeline - Random Forest (Grid Search)\n",
    "parameters = {\n",
    "        'clf__n_estimators': [100, 200],\n",
    "        'clf__min_samples_split': [2, 4]\n",
    "    }\n",
    "\n",
    "ml_pipeline = GridSearchCV(pipeline, param_grid=parameters, cv=4)\n",
    "\n",
    "# Train the model using grid search\n",
    "ml_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Output best selection of parameters\n",
    "print(ml_pipeline.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0d254",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55266f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 87.48 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      3524\n",
      "           1       0.91      0.90      0.90      6643\n",
      "\n",
      "    accuracy                           0.87     10167\n",
      "   macro avg       0.86      0.86      0.86     10167\n",
      "weighted avg       0.88      0.87      0.87     10167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - Random Forest\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17130cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 83.64 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.70      0.75      3524\n",
      "           1       0.85      0.91      0.88      6643\n",
      "\n",
      "    accuracy                           0.84     10167\n",
      "   macro avg       0.83      0.80      0.81     10167\n",
      "weighted avg       0.83      0.84      0.83     10167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - AdaBoost\n",
    "y_pred2 = pipeline2.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred2 == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33f33c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 83.97 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      3524\n",
      "           1       0.86      0.90      0.88      6643\n",
      "\n",
      "    accuracy                           0.84     10167\n",
      "   macro avg       0.83      0.81      0.82     10167\n",
      "weighted avg       0.84      0.84      0.84     10167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - Gradient Boosting\n",
    "y_pred3 = pipeline3.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred3 == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac746310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 86.41 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.79      0.80      3524\n",
      "           1       0.89      0.90      0.90      6643\n",
      "\n",
      "    accuracy                           0.86     10167\n",
      "   macro avg       0.85      0.85      0.85     10167\n",
      "weighted avg       0.86      0.86      0.86     10167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - Neural Network\n",
    "y_pred4 = pipeline4.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred4 == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "283d5e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 87.25 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.81      0.81      3524\n",
      "           1       0.90      0.91      0.90      6643\n",
      "\n",
      "    accuracy                           0.87     10167\n",
      "   macro avg       0.86      0.86      0.86     10167\n",
      "weighted avg       0.87      0.87      0.87     10167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - Ensemble Stacking\n",
    "y_pred5 = pipeline5.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred5 == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7052a5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Using backend ThreadingBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=8)]: Done  25 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 87.48 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82      3524\n",
      "           1       0.91      0.90      0.90      6643\n",
      "\n",
      "    accuracy                           0.87     10167\n",
      "   macro avg       0.86      0.86      0.86     10167\n",
      "weighted avg       0.88      0.87      0.87     10167\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done 146 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data - Random Forest (Grid Search)\n",
    "y_pred_final = ml_pipeline.predict(X_test)\n",
    "\n",
    "# show metrics\n",
    "print(f'Overall accuracy: {np.round(100 * (y_pred_final == y_test).mean().mean(), 2)} %')\n",
    "print(classification_report(y_test.values, y_pred_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697a251",
   "metadata": {},
   "source": [
    "In summary:\n",
    "- AdaBoost & Gradient Boosting takes a short time, but poor accuracy\n",
    "- Ensemble Stacking & Neural Network achieves good accuracy levels, but take a long time\n",
    "- Random Forest not only achieves good accuracy levels on par with more complex model, it achieved in a short time as AdaBoost & Gradient Boosting. Therefore, it is chosen along with GridSearch for further model improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479328e",
   "metadata": {},
   "source": [
    "# Output Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1236c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ml_pipeline, open(cd + '\\\\sentiment_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c43c4dba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-71c30c1f2669>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcd\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\\\sentiment_classifier.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(pipeline, open(cd + '\\\\sentiment_classifier.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
